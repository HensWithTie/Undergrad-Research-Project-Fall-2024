{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.cache_utils import DynamicCache\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, input_ids: torch.Tensor, past_key_values, max_new_tokens: int = 50) -> torch.Tensor:\n",
    "    device = model.model.embed_tokens.weight.device\n",
    "    origin_len = input_ids.shape[-1]\n",
    "    input_ids = input_ids.to(device)\n",
    "    output_ids = input_ids.clone()\n",
    "    next_token = input_ids\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            out = model(\n",
    "                input_ids=next_token,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True\n",
    "            )\n",
    "            logits = out.logits[:, -1, :]\n",
    "            token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "            output_ids = torch.cat([output_ids, token], dim=-1)\n",
    "            past_key_values = out.past_key_values\n",
    "            next_token = token.to(device)\n",
    "\n",
    "            if model.config.eos_token_id is not None and token.item() == model.config.eos_token_id:\n",
    "                break\n",
    "    return output_ids[:, origin_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kv_cache(model, tokenizer, prompt: str) -> DynamicCache:\n",
    "    device = model.model.embed_tokens.weight.device\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    cache = DynamicCache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model(\n",
    "            input_ids=input_ids,\n",
    "            past_key_values=cache,\n",
    "            use_cache=True\n",
    "        )\n",
    "    return cache\n",
    "\n",
    "def clean_up(cache: DynamicCache, origin_len: int):\n",
    "    for i in range(len(cache.key_cache)):\n",
    "        cache.key_cache[i] = cache.key_cache[i][:, :, :origin_len, :]\n",
    "        cache.value_cache[i] = cache.value_cache[i][:, :, :origin_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d050f5747764e22932521d55bf9f675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e954619a5a1549dc957f51f793288861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   7%|6         | 682M/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/repos/ea/00/ea00943d992c7851ad9f4f4bd094a0397fb5087e0f7cba4ef003018963ea07e3/a464228d9c9427bf035cfd5a2e18bf0494d7231bfacc478ec5fc9f57a612d051?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00001-of-00002.safetensors%3B+filename%3D%22model-00001-of-00002.safetensors%22%3B&Expires=1743016682&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MzAxNjY4Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9lYS8wMC9lYTAwOTQzZDk5MmM3ODUxYWQ5ZjRmNGJkMDk0YTAzOTdmYjUwODdlMGY3Y2JhNGVmMDAzMDE4OTYzZWEwN2UzL2E0NjQyMjhkOWM5NDI3YmYwMzVjZmQ1YTJlMThiZjA0OTRkNzIzMWJmYWNjNDc4ZWM1ZmM5ZjU3YTYxMmQwNTE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=U5HFtYFbTXTcHBsLBGcXjFqZfezcWqwUYjFJ1fJJnawNWts2yXH6APxLoVvNBdGFobLSGHv%7E3MOZoZVuwY3mfxX8BA080X0tSrflV22Nh-ICTOB6ncRsN%7E5S9K%7E3dP2bDdnkgV4pdme%7Errf6uiyn5oRUxwqdmjnfA8FF54qOZjbyNjW47QGXzzXfr3sa1C62C2RBQc4pCHBBcJ7apOXJvpHFTfPnc1ntfZvpFa1C0qSfgaSO-NwEafGfqDv7kAk3hWFjJZIkUAE8ZKVp8K-ZJVPO7Vs1KXH42OMh%7E4NTLSldxSibr03GG3wkiCLebwGl2yqZ2WEpeaS4Jo8KFhoteQ__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef8b73543a442fc86995be000f65843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:  32%|###2      | 3.20G/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jjtra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Jjtra\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a2cfa172b914f9eb6a7ad4097bbd551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2182c14e0fa745629fd66c7c041a4e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed530a7137554cfabea7ce73d7d97314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n",
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "You can't move a model that has some modules offloaded to cpu or disk.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      4\u001b[0m     model_name,\n\u001b[0;32m      5\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16 \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_zRhGQHyffLdHyxsfURFPeufarlhwIgeXMK\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Jjtra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\big_modeling.py:458\u001b[0m, in \u001b[0;36mdispatch_model.<locals>.add_warning.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 458\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt move a model that has some modules offloaded to cpu or disk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: You can't move a model that has some modules offloaded to cpu or disk."
     ]
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=\"\", trust_remote_code=True) #INSERT HF TOKEN IN \"\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    token=\"\" #INSERT HF TOKEN IN \"\"\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(f\"Loaded {model_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stime = time.time()\n",
    "with open(\"Jon.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    doc_text = f.read()\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "<|system|>\n",
    "You are an assistant who provides concise factual answers. You will read the text file\n",
    "and respond with a max of 3 sentences. If you do not know, say \"I don't know\".\n",
    "<|user|>\n",
    "Context:\n",
    "{doc_text}\n",
    "Question:\n",
    "\"\"\".strip()\n",
    "\n",
    "ronan_cache = get_kv_cache(model, tokenizer, system_prompt)\n",
    "origin_len = ronan_cache.key_cache[0].shape[-2]\n",
    "print(\"KV cache built.\")\n",
    "etime = time.time()\n",
    "ttime = etime - stime\n",
    "print(f\"total time: {ttime:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stime = time.time()\n",
    "question1 = \"What is the index 1 called?\" #Change question to user input\n",
    "clean_up(ronan_cache, origin_len)\n",
    "input_ids_q1 = tokenizer(question1 + \"\\n\", return_tensors=\"pt\").input_ids.to(device)\n",
    "gen_ids_q1 = generate(model, input_ids_q1, ronan_cache)\n",
    "answer1 = tokenizer.decode(gen_ids_q1[0], skip_special_tokens=True)\n",
    "print(\"Q1:\", question1)\n",
    "print(\"A1:\", answer1)\n",
    "\n",
    "etime = time.time()\n",
    "ttime = etime - stime\n",
    "print(f\"total time: {ttime:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stime = time.time()\n",
    "question2 = \"What does Jonathan like?\" #Change question to user input\n",
    "clean_up(ronan_cache, origin_len)\n",
    "input_ids_q2 = tokenizer(question2 + \"\\n\", return_tensors=\"pt\").input_ids.to(device)\n",
    "gen_ids_q2 = generate(model, input_ids_q2, ronan_cache)\n",
    "answer2 = tokenizer.decode(gen_ids_q2[0], skip_special_tokens=True)\n",
    "print(\"Q2:\", question2)\n",
    "print(\"A2:\", answer2)\n",
    "\n",
    "etime = time.time()\n",
    "ttime = etime - stime\n",
    "print(f\"total time: {ttime:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stime = time.time()\n",
    "question3 = \"What is the document about?\" #Change question to user input\n",
    "clean_up(ronan_cache, origin_len)\n",
    "input_ids_q3 = tokenizer(question3 + \"\\n\", return_tensors=\"pt\").input_ids.to(device)\n",
    "gen_ids_q3 = generate(model, input_ids_q3, ronan_cache)\n",
    "answer3 = tokenizer.decode(gen_ids_q3[0], skip_special_tokens=True)\n",
    "print(\"Q3:\", question3)\n",
    "print(\"A3:\", answer3)\n",
    "\n",
    "etime = time.time()\n",
    "ttime = etime - stime\n",
    "print(f\"total time: {ttime:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stime = time.time()\n",
    "question4 = \"Does Jonathan like Magic the Gathering?\" #Change question to user input\n",
    "clean_up(ronan_cache, origin_len)\n",
    "input_ids_q4 = tokenizer(question4 + \"\\n\", return_tensors=\"pt\").input_ids.to(device)\n",
    "gen_ids_q4 = generate(model, input_ids_q4, ronan_cache)\n",
    "answer4 = tokenizer.decode(gen_ids_q4[0], skip_special_tokens=True)\n",
    "print(\"Q4:\", question4)\n",
    "print(\"A4:\", answer4)\n",
    "\n",
    "etime = time.time()\n",
    "ttime = etime - stime\n",
    "print(f\"total time: {ttime:.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
